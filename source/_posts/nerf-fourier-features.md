---
title: nerf fourier features
date: 2024-12-15 17:44:48
tags: "nerf"
categories: "nerf"
comments: false
---

[来源](https://zhuanlan.zhihu.com/p/10748639711)


和nerf类似，利用MLP来拟合图像  
也即MLP输入为2D坐标，输出为此处的RGB颜色  
实验发现，MLP无法拟合高频数据，如图块边缘  
则问题是：如何让MLP拟合高频信息   


# 核回归
NTK（nerual tangent kernel）神经正切核  
NTK 是一种特殊的kernel regression方法  
假如有 $ (x_i,y_i) $ 的训练数据  
再定义一个相似度计算$ K(x_1,x_2) $  
那么对于任意输入x而言  
$ w_i = \frac{K(x_i,x)}{\sum_{i=1}^{n}K(x_i,x)} $  
$ f(x)=\sum_{i=1}^{n}w_iy_i $  
也即，对于新输入x，计算与训练数据的相似度然后归一化，再计算基于y的加权和  
这个相似度计算被成为kernel

#### 利用实验来做一个简单的一维函数拟合回归
利用正态分布的概率密度函数来表示核  
可以看到标准差越小，模型越倾向于过拟合，标准差越大，曲线会逐渐平滑  
解释：标准差过小，每个训练数据的影响范围就较小，那么测试样本会更容易受到较近的训练数据影响  
标准差过大，每个训练数据的影响会趋于平缓，也就是所有训练数据影响都差不多，也就无法做到拟合功能了  

# 神经正切核
在一些特殊情况下，MLP的最终优化结果可以用一个核回归来表示，这些能表示神经网络学习结果的核被称为NTK  

# 两件事
神经网络最终的收敛效果可以由简单的核回归决定，核回归的重点是定义了两个输入之间的相似指标  
表示神经网络的核回归相似度指标是NTK，NTK又取决于两个输入的内积  

### 傅里叶特征的平移不变性
通过cos和sin来制作位置编码  
同时，NTK只取决于输入间的内积，内积依赖于v1-v2的相对距离   
也即，NTK决定了神经网络学习的结果，神经网络收敛结果取决于输入间的相对距离  

新的NTK被写为：  
![img.png](/uploads/img.png)  
也就是傅里叶变换的形式，j较大的项表示NTK的高频分量，可以通过修改$ a_j $来调整NTK的频域特征。  
所以，位置编码像是在模拟傅里叶变换，被叫做傅里叶特征。  


## 随机傅里叶特征
理论上，傅里叶特征的长度要和原图像像素数一样，同时要有不同方向的频率，但是这样会产生大量参数  
'Random features for large-scale kernel machines'表明，不需要密集采样，只需要稀疏采样即可  
也即，我们可以从某个分布里面随机采样m个频率$ b_j $，从哪个分布采样无所谓，重要的是标准差，标准差决定了网络拟合高频信息的能力  
记住，傅里叶特征只在MLP拟合连续数据的任务有效，和transformer的位置编码无关  

## 代码
代码里的 fourier_basis表示傅里叶特征的频率，也就是b  
scale为标准差，正太分布的标准差  

## 结论
1. NTK的理论分析后，位置编码是一种特殊的傅里叶特征，其具有平移不变性。因此，我们可以通过调整傅里叶特征的参数来调整卷积的带宽，也即调整网络对不同频率的关注程度，使得网络不会忽略高频信息。  
2. 傅里叶特征不需要密集采样，只需要从任一分布随机稀疏采样，影响效果的关键是采样分布的标准差，其决定了傅里叶特征的带宽也即网络能否关注到高频信息。  


